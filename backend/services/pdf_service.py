import os
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import asyncio
from concurrent.futures import ThreadPoolExecutor
from loguru import logger

try:
    from pypdf import PdfReader
    from pypdf.errors import PdfReadError
except ImportError:
    logger.warning("pypdf not installed. PDF parsing will not work.")
    PdfReader = None

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

from ..config.settings import settings
from ..config.database import AsyncSessionLocal, Document as DocumentModel


class PDFParsingService:
    """PDF íŒŒì‹± ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.chunk_size = settings.chunk_size
        self.chunk_overlap = settings.chunk_overlap
        self.upload_path = Path(settings.upload_path)
        self.processed_path = Path(settings.processed_path)
        self.executor = ThreadPoolExecutor(max_workers=2)

        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.upload_path.mkdir(parents=True, exist_ok=True)
        self.processed_path.mkdir(parents=True, exist_ok=True)

    async def save_uploaded_file(self, file_content: bytes, filename: str) -> str:
        """ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥"""
        try:
            # íŒŒì¼ëª… í•´ì‹œ ìƒì„± (ì¤‘ë³µ ë°©ì§€)
            file_hash = hashlib.md5(file_content).hexdigest()
            safe_filename = f"{file_hash}_{filename}"
            file_path = self.upload_path / safe_filename

            # íŒŒì¼ ì €ì¥
            with open(file_path, "wb") as f:
                f.write(file_content)

            logger.info(f"íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_path}")
            return str(file_path)

        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ({filename}): {e}")
            raise

    async def extract_text_from_pdf(self, file_path: str) -> Tuple[str, Dict[str, Any]]:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not PdfReader:
            raise ImportError("pypdf ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        try:
            # ë¹„ë™ê¸°ì ìœ¼ë¡œ PDF ì½ê¸°
            text, metadata = await asyncio.get_event_loop().run_in_executor(
                self.executor, self._extract_text_sync, file_path
            )
            return text, metadata

        except Exception as e:
            logger.error(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ({file_path}): {e}")
            raise

    def _extract_text_sync(self, file_path: str) -> Tuple[str, Dict[str, Any]]:
        """ë™ê¸°ì  PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë‚´ë¶€ í•¨ìˆ˜)"""
        try:
            reader = PdfReader(file_path)

            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = {
                "page_count": len(reader.pages),
                "title": reader.metadata.get("/Title", ""),
                "author": reader.metadata.get("/Author", ""),
                "subject": reader.metadata.get("/Subject", ""),
                "creator": reader.metadata.get("/Creator", ""),
                "producer": reader.metadata.get("/Producer", ""),
                "creation_date": reader.metadata.get("/CreationDate", ""),
                "modification_date": reader.metadata.get("/ModDate", ""),
            }

            # ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            full_text = ""
            page_texts = []

            for i, page in enumerate(reader.pages):
                try:
                    page_text = page.extract_text()
                    if page_text.strip():
                        # í…ìŠ¤íŠ¸ ì •ê·œí™”
                        normalized_text = self._normalize_text(page_text)
                        page_texts.append({
                            "page": i + 1,
                            "text": normalized_text,
                            "raw_text": page_text
                        })
                        full_text += f"\n\n--- Page {i + 1} ---\n{normalized_text}"

                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ {i + 1} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue

            metadata["pages"] = page_texts
            metadata["total_characters"] = len(full_text)

            if not full_text.strip():
                raise ValueError("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº”ëœ ë¬¸ì„œì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")

            return full_text.strip(), metadata

        except PdfReadError as e:
            raise ValueError(f"PDF íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        except Exception as e:
            raise RuntimeError(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        if not text:
            return ""

        # ë¦¬ê°€ì²˜ ë¬¸ì ë³€í™˜
        ligature_map = {
            "\ufb00": "ff", "\ufb01": "fi", "\ufb02": "fl",
            "\ufb03": "ffi", "\ufb04": "ffl"
        }

        for ligature, replacement in ligature_map.items():
            text = text.replace(ligature, replacement)

        # í•˜ì´í”ˆ ì—°ê²°ëœ ë‹¨ì–´ ì²˜ë¦¬ (ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬ëœ ë‹¨ì–´ ì—°ê²°)
        import re
        text = re.sub(r"(\w)-\s*\n\s*(\w)", r"\1\2", text)

        # ë‹¤ì¤‘ ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ
        text = re.sub(r"\s+", " ", text)

        # ë‹¤ì¤‘ ì¤„ë°”ê¿ˆì„ ë‹¨ì¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ
        text = re.sub(r"\n+", "\n", text)

        return text.strip()

    async def chunk_text(
        self,
        text: str,
        document_id: str = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        try:
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", ". ", " ", ""]
            )

            loop = asyncio.get_event_loop()

            # ë¬¸ì„œë³„ ê³µí†µ ë©”íƒ€ë°ì´í„° êµ¬ì„±
            common_metadata: Dict[str, Any] = {}
            if document_id:
                common_metadata["document_id"] = document_id

            # ë¬¸ì„œ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            document_info = None
            if document_id:
                try:
                    from ..config.database import AsyncSessionLocal
                    from ..config.database import Document as DBDocument
                    from sqlalchemy import select

                    async with AsyncSessionLocal() as db:
                        result = await db.execute(select(DBDocument).where(DBDocument.id == document_id))
                        document_info = result.scalar_one_or_none()
                except Exception as e:
                    logger.warning(f"ë¬¸ì„œ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")

            if document_info:
                if document_info.original_name:
                    common_metadata["filename"] = document_info.original_name
                if document_info.document_type:
                    common_metadata["document_type"] = document_info.document_type
                if document_info.product_family:
                    common_metadata["product_family"] = document_info.product_family
                if document_info.product_model:
                    common_metadata["product_model"] = document_info.product_model

            page_entries = []
            if metadata and metadata.get("pages"):
                page_entries = [page for page in metadata["pages"] if page.get("text")]

            if page_entries:
                texts: List[str] = []
                metadatas: List[Dict[str, Any]] = []

                for page in page_entries:
                    base_metadata = dict(common_metadata)
                    if page.get("page") is not None:
                        base_metadata["page_number"] = int(page.get("page"))
                    texts.append(page.get("text", ""))
                    metadatas.append(base_metadata)

                def create_documents_with_metadata():
                    return splitter.create_documents(texts, metadatas=metadatas)

                chunks = await loop.run_in_executor(
                    self.executor,
                    create_documents_with_metadata
                )
            else:
                base_metadata = dict(common_metadata)

                def create_single_document():
                    return splitter.create_documents([text], metadatas=[base_metadata])

                chunks = await loop.run_in_executor(
                    self.executor,
                    create_single_document
                )

            # ì²­í¬ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for i, chunk in enumerate(chunks):
                chunk.metadata.setdefault("document_id", document_id)
                if document_info:
                    if document_info.original_name:
                        chunk.metadata.setdefault("filename", document_info.original_name)
                    if document_info.document_type:
                        chunk.metadata.setdefault("document_type", document_info.document_type)
                    if document_info.product_family:
                        chunk.metadata.setdefault("product_family", document_info.product_family)
                    if document_info.product_model:
                        chunk.metadata.setdefault("product_model", document_info.product_model)

                chunk.metadata["chunk_index"] = i
                chunk.metadata["chunk_size"] = len(chunk.page_content)
                if "page_number" in chunk.metadata and chunk.metadata["page_number"] is not None:
                    try:
                        chunk.metadata["page_number"] = int(chunk.metadata["page_number"])
                    except (TypeError, ValueError):
                        chunk.metadata["page_number"] = None

            logger.info(f"í…ìŠ¤íŠ¸ë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
            return chunks

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤íŒ¨: {e}")
            raise

    async def process_pdf_file(self, file_path: str, document_id: str) -> Tuple[List[Document], Dict[str, Any]]:
        """PDF íŒŒì¼ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        try:
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            logger.info(f"PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {file_path}")
            text, metadata = await self.extract_text_from_pdf(file_path)

            # 2. í…ìŠ¤íŠ¸ ì²­í‚¹
            logger.info(f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œì‘")
            chunks = await self.chunk_text(text, document_id, metadata)

            # 3. ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì €ì¥ (ì˜µì…˜)
            processed_file_path = self.processed_path / f"{document_id}.txt"
            with open(processed_file_path, "w", encoding="utf-8") as f:
                f.write(text)

            logger.info(f"PDF ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

            return chunks, metadata

        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨ ({file_path}): {e}")
            raise

    async def extract_tables_from_pdf(self, file_path: str) -> List[Dict[str, Any]]:
        """PDFì—ì„œ í…Œì´ë¸” ì¶”ì¶œ (í–¥í›„ í™•ì¥ìš©)"""
        # TODO: pdfplumberë‚˜ tabula-pyë¥¼ ì‚¬ìš©í•œ í…Œì´ë¸” ì¶”ì¶œ êµ¬í˜„
        logger.warning("í…Œì´ë¸” ì¶”ì¶œ ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    async def detect_document_structure(self, text: str) -> Dict[str, Any]:
        """ë¬¸ì„œ êµ¬ì¡° ê°ì§€ (ì„¹ì…˜, ì œëª© ë“±)"""
        # TODO: ì •ê·œì‹ì´ë‚˜ ML ëª¨ë¸ì„ ì‚¬ìš©í•œ êµ¬ì¡° ê°ì§€
        import re

        # ê°„ë‹¨í•œ ì„¹ì…˜ ê°ì§€
        sections = []
        section_patterns = [
            r"^\s*(\d+\.?\s*.+?)$",  # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì œëª©
            r"^\s*([A-Z][^a-z\n]{10,})$",  # ëŒ€ë¬¸ì ì œëª©
            r"^\s*(Introduction|Abstract|Conclusion|References?|Specification|Features?|Overview).*$",
        ]

        for pattern in section_patterns:
            matches = re.finditer(pattern, text, re.MULTILINE | re.IGNORECASE)
            for match in matches:
                sections.append({
                    "title": match.group(1).strip(),
                    "start_pos": match.start(),
                    "pattern": pattern
                })

        return {
            "sections": sections,
            "has_toc": "table of contents" in text.lower() or "ëª©ì°¨" in text,
            "has_index": "index" in text.lower() or "ìƒ‰ì¸" in text,
        }

    def cleanup_temp_files(self, older_than_hours: int = 24):
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        import time

        try:
            now = time.time()
            cutoff = older_than_hours * 3600

            for file_path in self.processed_path.glob("*.txt"):
                if now - file_path.stat().st_mtime > cutoff:
                    file_path.unlink()
                    logger.info(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {file_path}")

        except Exception as e:
            logger.error(f"ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    async def process_pdf_background(self, file_path: str, document_id: str, vector_service):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ PDF ì²˜ë¦¬ ë° ë²¡í„°í™” - ë…¼ë¸”ë¡œí‚¹ ì²˜ë¦¬"""
        try:
            # ì²˜ë¦¬ ìƒíƒœë¥¼ processingìœ¼ë¡œ ì—…ë°ì´íŠ¸
            await self._update_document_status(document_id, "processing")

            # PDF ì²˜ë¦¬
            logger.info(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ PDF ì²˜ë¦¬ ì‹œì‘: {document_id}")
            chunks, metadata = await self.process_pdf_file(file_path, document_id)

            # chunksëŠ” ì´ë¯¸ LangChain Document ê°ì²´ë“¤ì´ë¯€ë¡œ ë°”ë¡œ ì‚¬ìš©
            documents = chunks

            # ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± (ì²­í¬ë³„ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ )
            logger.info(f"ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ì‹œì‘: {len(documents)}ê°œ ì²­í¬")

            # ë°°ì¹˜ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            batch_size = 50
            total_batches = (len(documents) + batch_size - 1) // batch_size

            for i in range(0, len(documents), batch_size):
                batch = documents[i:i + batch_size]
                batch_num = (i // batch_size) + 1

                logger.info(f"ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch)}ê°œ ì²­í¬)")

                if i == 0:
                    # ì²« ë²ˆì§¸ ë°°ì¹˜: ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
                    success = await vector_service.create_index_from_documents(batch)
                else:
                    # ì´í›„ ë°°ì¹˜: ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€
                    success = await vector_service.add_documents(batch)

                if not success:
                    logger.error(f"âŒ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹¤íŒ¨")
                    await self._update_document_status(document_id, "failed")
                    return

                # ì§„í–‰ ìƒí™© ë¡œê¹…
                progress = (batch_num / total_batches) * 100
                logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}%")

            # ì„±ê³µ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
            await self._update_document_status(document_id, "completed")
            logger.info(f"âœ… ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì™„ë£Œ: {document_id} ({len(documents)}ê°œ ì²­í¬)")

        except Exception as e:
            logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ ({document_id}): {e}")
            await self._update_document_status(document_id, "failed")

    async def _update_document_status(self, document_id: str, status: str):
        """ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            from ..config.database import AsyncSessionLocal
            from sqlalchemy import update
            from ..config.database import Document as DBDocument

            async with AsyncSessionLocal() as session:
                stmt = update(DBDocument).where(
                    DBDocument.id == document_id
                ).values(processing_status=status)
                await session.execute(stmt)
                await session.commit()

        except Exception as e:
            logger.error(f"ë¬¸ì„œ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ({document_id} -> {status}): {e}")


# ì „ì—­ PDF ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_pdf_service: Optional[PDFParsingService] = None

def get_pdf_service() -> PDFParsingService:
    """PDF ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _pdf_service
    if _pdf_service is None:
        _pdf_service = PDFParsingService()
    return _pdf_service
